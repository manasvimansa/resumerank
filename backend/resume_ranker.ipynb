{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d52080-1c43-49cf-9e26-79927c6ed8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load skills from file\n",
    "with open(\"skills.txt\") as f:\n",
    "    skill_list = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "# Preprocess skills to allow multi-word matching\n",
    "skill_set = set(skill_list)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# NO print(...) calls below. Instead, wrap in a function that returns data.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_skills_from_resume_text(resume_text):\n",
    "    \"\"\"\n",
    "    Given resume_text (string), return a Python list of matched skills.\n",
    "    This function can be called by a Flask route to produce JSON output.\n",
    "    \"\"\"\n",
    "    skills_found = extract_skills(resume_text)\n",
    "    return list(skills_found)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb5732-24a5-4d7d-939e-05f0ac263352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load skills from file\n",
    "with open(\"skills.txt\") as f:\n",
    "    skill_list = [line.strip().lower() for line in f.readlines()]\n",
    "skill_set = set(skill_list)\n",
    "\n",
    "# Skill extraction function\n",
    "def extract_skills(text):\n",
    "    doc = nlp(text.lower())\n",
    "    extracted_skills = set()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.strip().lower()\n",
    "        if chunk_text in skill_set:\n",
    "            extracted_skills.add(chunk_text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in skill_set:\n",
    "            extracted_skills.add(token.text)\n",
    "    \n",
    "    return extracted_skills\n",
    "\n",
    "# PDF resume text extraction\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# ─── Modified process_resume for Flask ────────────────────────────────────────\n",
    "def process_resume(resume_path, job_description):\n",
    "    \"\"\"\n",
    "    Given a path to a PDF resume and a job description string, return a dict:\n",
    "      - 'skills':   list of matched skills (from skills.txt)\n",
    "      - 'resume_text_preview': first 1000 characters of the resume text\n",
    "      - 'error':    only present if the resume file was not found\n",
    "    \"\"\"\n",
    "    if not os.path.exists(resume_path):\n",
    "        return {\"error\": \"File not found.\"}\n",
    "\n",
    "    # 1) Extract full resume text\n",
    "    resume_text = extract_text_from_pdf(resume_path)\n",
    "\n",
    "    # 2) Extract skills (using skills.txt match)\n",
    "    skills_found = extract_skills(resume_text)\n",
    "\n",
    "    # Return as a JSON-serializable dict\n",
    "    return {\n",
    "        \"resume_text_preview\": resume_text[:1000],\n",
    "        \"skills\": list(skills_found)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c656b02-c494-4f06-992d-e09654e4665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_skills(resume_text):\n",
    "    \"\"\"\n",
    "    From resume_text, return a deduplicated list of all NOUN/PROPN tokens\n",
    "    (excluding stop words). No printing—just return a list.\n",
    "    \"\"\"\n",
    "    doc = nlp(resume_text)\n",
    "    skills = [\n",
    "        token.text\n",
    "        for token in doc\n",
    "        if token.pos_ in (\"NOUN\", \"PROPN\") and not token.is_stop\n",
    "    ]\n",
    "    # Convert to set (to dedupe) and then back to list\n",
    "    return list(set(skills))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4de726-1fab-46d3-91de-0c80e13f6264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_entities(resume_text):\n",
    "    \"\"\"\n",
    "    From resume_text, return a list of (entity_text, entity_label) tuples.\n",
    "    \"\"\"\n",
    "    doc = nlp(resume_text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e7dbe-9825-44ea-9f54-04979ecb02ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize_text(text, n=3):\n",
    "    \"\"\"\n",
    "    Return a summary (n sentences) of the provided text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    word_freq = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in STOP_WORDS and word.text.lower() not in punctuation:\n",
    "            word_freq[word.text.lower()] = word_freq.get(word.text.lower(), 0) + 1\n",
    "\n",
    "    sentence_strength = {}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_freq:\n",
    "                sentence_strength[sent] = sentence_strength.get(sent, 0) + word_freq[word.text.lower()]\n",
    "\n",
    "    top_sentences = nlargest(n, sentence_strength, key=sentence_strength.get)\n",
    "    return ' '.join([str(s) for s in top_sentences])\n",
    "\n",
    "# Example usage inside a Flask route (no print statements):\n",
    "# summary = summarize_text(resume_text, 3)\n",
    "# return jsonify({\"summary\": summary})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1ec41-401f-4eed-9b1c-be487b05728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Given a string `text`, return a set of all NOUN/PROPN tokens\n",
    "    (lowercased, alphabetic, and not stop words).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return {\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if token.pos_ in (\"NOUN\", \"PROPN\") and not token.is_stop and token.is_alpha\n",
    "    }\n",
    "\n",
    "def get_keywords_from_text(text):\n",
    "    \"\"\"\n",
    "    Flask can call this to get keywords as a JSON‐serializable list.\n",
    "    \"\"\"\n",
    "    return list(extract_keywords(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922759c-61f1-4e70-8bc6-3bf1c5bbb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from text (you can keep this if you still need it elsewhere)\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    return {\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if token.pos_ in (\"NOUN\", \"PROPN\") and not token.is_stop and token.is_alpha\n",
    "    }\n",
    "\n",
    "# Existing skill‐extraction function (no changes needed here)\n",
    "def extract_skills(text):\n",
    "    doc = nlp(text.lower())\n",
    "    extracted_skills = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.strip().lower()\n",
    "        if chunk_text in skill_set:\n",
    "            extracted_skills.add(chunk_text)\n",
    "    for token in doc:\n",
    "        if token.text in skill_set:\n",
    "            extracted_skills.add(token.text)\n",
    "    return extracted_skills\n",
    "\n",
    "# ─── Wrapped into a single function ─────────────────────────────────────────────\n",
    "\n",
    "def calculate_skill_match(job_description, resume_text):\n",
    "    \"\"\"\n",
    "    Given job_description and resume_text, return a dict:\n",
    "      - 'job_skills':    set of skills extracted from job_description\n",
    "      - 'resume_skills': set of skills extracted from resume_text\n",
    "      - 'matched_skills': the intersection of the two sets\n",
    "      - 'skill_score':   a float between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    jd_skills = extract_skills(job_description)\n",
    "    resume_skills = extract_skills(resume_text)\n",
    "    matched_skills = jd_skills & resume_skills\n",
    "\n",
    "    skill_score = (\n",
    "        len(matched_skills) / len(jd_skills)\n",
    "        if jd_skills\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"job_skills\": list(jd_skills),\n",
    "        \"resume_skills\": list(resume_skills),\n",
    "        \"matched_skills\": list(matched_skills),\n",
    "        \"skill_score\": round(skill_score, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96a0f5-18dd-4410-b6d5-218ab977bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_score(resume_text, job_description):\n",
    "    \"\"\"\n",
    "    Return the final resume score (rounded to two decimals) instead of printing it.\n",
    "    \"\"\"\n",
    "    final_score = calculate_resume_score(resume_text, job_description)\n",
    "    return round(final_score, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b80a37-e4fe-4cb2-8f64-ad0792b27fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_top_n_keywords(text, n=10):\n",
    "    \"\"\"\n",
    "    Return a set of the top‐n keywords (by TF-IDF score) from the given text.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    keywords = sorted(zip(feature_names, scores), key=lambda x: -x[1])[:n]\n",
    "    return set(k for k, _ in keywords)\n",
    "\n",
    "def get_similarity_score(text1, text2):\n",
    "    \"\"\"\n",
    "    Return the cosine similarity between text1 and text2 based on TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform([text1, text2])\n",
    "    return cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "def calculate_resume_score(resume_text, job_text):\n",
    "    \"\"\"\n",
    "    Compute a weighted final score for a resume: \n",
    "      - 50% from TF-IDF keyword overlap (resume vs. job)\n",
    "      - 30% from a simple experience heuristic\n",
    "      - 20% from a simple education heuristic\n",
    "    \"\"\"\n",
    "    # Skill score via TF-IDF top-n keywords\n",
    "    resume_keywords = extract_top_n_keywords(resume_text)\n",
    "    job_keywords = extract_top_n_keywords(job_text)\n",
    "    skill_score = (\n",
    "        len(resume_keywords & job_keywords) / len(job_keywords)\n",
    "        if job_keywords\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Experience score (simple rule: check for “2 years”, “3 years”, or “experience”)\n",
    "    experience_score = 1.0 if any(\n",
    "        x in resume_text.lower() for x in [\"2 years\", \"3 years\", \"experience\"]\n",
    "    ) else 0.5\n",
    "\n",
    "    # Education score (check for “b.tech”, “bachelor”, “graduation”)\n",
    "    education_score = 1.0 if any(\n",
    "        x in resume_text.lower() for x in [\"b.tech\", \"bachelor\", \"graduation\"]\n",
    "    ) else 0.0\n",
    "\n",
    "    final_score = (0.5 * skill_score) + (0.3 * experience_score) + (0.2 * education_score)\n",
    "    return final_score\n",
    "\n",
    "def get_resume_score(resume_text, job_description):\n",
    "    \"\"\"\n",
    "    Wrapper for Flask: returns the final score (rounded to two decimals).\n",
    "    \"\"\"\n",
    "    score = calculate_resume_score(resume_text, job_description)\n",
    "    return round(score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf1ab7-f575-4898-9ec3-c9652c214f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
